<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

  <meta content="width=device-width, initial-scale=1" name="viewport">
  <title>Brandon Y. Feng</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="./css/fontawesome.all.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
  <link href="./css/bulma.min.css" rel="stylesheet">
  <link href="./css/resume.css" rel="stylesheet">
  <link rel="icon" href="images/battery-charging.svg" type="image/svg+xml" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
</head>

<body>

  <section>
    <div class="container is-max-desktop">
      <div class="columns is-vcentered">
        <div class="column">
          <h1 class="title is-3">Brandon Yushan Feng</h1>
          <h1 class="subtitle is-5">Ph.D. Candidate in Computer Science</h1>
        </div>
        <div class="column is-narrow portrait-column">
          <img class="portrait-image" src="./images/headshot.jpg"/>
        </div>
        <div class="column has-text-right">
          <p>
            <a href="mailto:yfeng97@umd.edu">
              <i class="fas fa-envelope"></i>
              yfeng97@umd.edu
            </a>
          </p>
        </div>
      </div>

      <div class="content">
        <h2>Education</h2>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <b>University of Maryland</b><br />
            Ph.D. in Computer Science.
          </div>
          <div class="column is-one-fifths has-text-right">
            2019 - 2023
            <br />
            College Park, MD
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <b>University of Virginia</b><br />
            M.S. in Statistics.
          </div>
          <div class="column is-one-fifths has-text-right">
            2018 - 2019
            <br />
            Charlottesville, VA
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <b>University of Virginia</b><br />
            B.A. in Computer Science. B.A. in Statistics.
          </div>
          <div class="column is-one-fifths has-text-right">
            2015 - 2018
            <br />
            Charlottesville, VA
          </div>
        </div>
        

        <h2>Employment</h2>

        
        <div class="columns">
          <div class="column is-four-fifths">
            <b>Google</b><br />
            Research Intern.
          </div>
          <div class="column is-one-fifths has-text-right">
            May - August 2022
            <br />
            San Francisco, CA
          </div>
        </div>
        

        <h2>Publications</h2>

        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href=""><b>VIINTER: View Interpolation With Implicit Neural Representations of Images</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://jsreddy.github.io/">S.   Jabbireddy</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://www.cs.umd.edu/~varshney/">A.   Varshney</a>
                
              </span>
              
            </div>
            View interpolation without 3D reconstruction or correspondence.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=https://sa2022.siggraph.org/en/>SIGGRAPH Asia, 2022</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href=""><b>Primary Ray-based Implicit Function</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://www.zhangyinda.com/">Y.   Zhang</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=9uxs6G4AAAAJ&hl=en">D.   Tang</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://ruofeidu.com/">R.   Du</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://www.cs.umd.edu/~varshney/">A.   Varshney</a>
                
              </span>
              
            </div>
            Efficient implicit 3D shape representation.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=https://eccv2022.ecva.net/>ECCV, 2022</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href=""><b>Benchmarking AlphaFold for Protein Complex Modeling Reveals Accuracy Determinants</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=ROPT34wAAAAJ&hl=en">R.   Yin</a>,
                
              </span>
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://www.cs.umd.edu/~varshney/">A.   Varshney</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=Y-VJVpgAAAAJ&hl=en">B. G.  Pierce</a>
                
              </span>
              
            </div>
            Interesting findings from evaluating AlphaFold2 on protein-protein docking, highlighting areas for future development using deep learning methods.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=https://onlinelibrary.wiley.com/journal/1469896x>Protein Science, 2022</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href=""><b>TurbuGAN: An Adversarial Learning Approach to Spatially-Varying Multiframe Blind Deconvolution with Applications to Imaging Through Turbulence</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block author-me">
                
                B. Y. Feng*,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=2x41vmkAAAAJ&hl=en">M.   Xie*</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="http://www.cs.umd.edu/~metzler/">C. A.  Metzler</a>
                
              </span>
              
            </div>
            Training GAN only on blurry images from a single scene to recover a sharp image without estimating the blur kernels or acquiring a large labelled dataset.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=>Preprint</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href="https://augmentariumlab.github.io/SIGNET"><b>SIGNET: Efficient Neural Representations For Light Fields</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://www.cs.umd.edu/~varshney/">A.   Varshney</a>
                
              </span>
              
            </div>
            Remarkable compression rates by representing light fields as neural network weights. Simple and compact formulation also supports angular interpolation to generate novel viewpoints.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=https://iccv2021.thecvf.com/home>ICCV, 2021</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href="https://ruofeidu.com/projects/gazechat/"><b>GazeChat: Enhancing Virtual Conferences with Gaze-aware 3D Photos</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=rPK3Ca0AAAAJ&hl=en">Z.   He</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="">K.   Wang</a>,
                
              </span>
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://ruofeidu.com/">R.   Du</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://cs.nyu.edu/~perlin/">K.   Perlin</a>
                
              </span>
              
            </div>
            Visualizing "who's looking at who" from static profile images as people always turn off their videos. Synthesized eye gazes are obtained by leveraging a general-purpose neural network.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=https://uist.acm.org/uist2021>UIST, 2021</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href=""><b>Deep Depth Estimation on 360-Degree Images With a Double Quaternion Loss</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="">W.   Yao</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="">Z.   Liu</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://www.cs.umd.edu/~varshney/">A.   Varshney</a>
                
              </span>
              
            </div>
            Training neural networks for 360&deg monocular depth and normal estimation. Proposed a novel approach of combining depth and normal as a double quaternion during loss computation.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=http://3dv2020.dgcv.nii.ac.jp/>3DV, 2020</a>
          </div>
        </div>
        
        <div class="columns">
          <div class="column is-four-fifths">
            <a href=""><b>A Self-Adaptive Network for Multiple Sclerosis Lesion Segmentation From Multi-Contrast MRI With Various Imaging Sequences</b></a>
            <div class="publication-authors">
              
              
              <span class="author-block author-me">
                
                B. Y. Feng,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=rlu_kJQAAAAJ&hl=en">H.   Pan</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=lvfWy6sAAAAJ&hl=en">C. H.  Meyer</a>,
                
              </span>
              
              
              <span class="author-block ">
                
                <a href="https://scholar.google.com/citations?user=0V31C1IAAAAJ&hl=en">X.   Feng</a>
                
              </span>
              
            </div>
            Neural networks with multimodal input disproportionately rely on certain modality while ignoring the rest. Developed a new strategy to overcome such bias and adapt to missing modalities.
          </div>
          <div class="column is-one-fifths has-text-right">
            <a href=https://biomedicalimaging.org/2019/>ISBI, 2019</a>
          </div>
        </div>
        
      </div>
    </div>
  </section>

</body>

</html>
